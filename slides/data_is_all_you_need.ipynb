{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fe9a52d-7b34-47f2-8943-f3ad81541a02",
   "metadata": {},
   "source": [
    "# Data is all you need\n",
    "\n",
    "## Miroslav  Jiřík"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f07916a-aa61-4722-a2b0-d78d00fea805",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "# Goal\n",
    "\n",
    "* How to split dataset\n",
    "* Important dataset\n",
    "* End to end training\n",
    "* Metacentrum - Czech Computational Grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b412be4c-672a-443d-926e-cbe5f2df8fd0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# How to split dataset\n",
    "\n",
    "## Training Dataset\n",
    "Training Dataset: The sample of data used to fit the model.\n",
    "The actual dataset that we use to train the model (weights and biases in the case of a Neural Network). The model sees and learns from this data.\n",
    "\n",
    "## Validation Dataset\n",
    "Validation Dataset: The sample of data used to provide an unbiased evaluation of a model fit on the training dataset while tuning model hyperparameters. The evaluation becomes more biased as skill on the validation dataset is incorporated into the model configuration.\n",
    "\n",
    "The validation set is used to evaluate a given model, but this is for frequent evaluation. \n",
    "\n",
    "## Test Dataset\n",
    "Test Dataset: The sample of data used to provide an unbiased evaluation of a final model fit on the training dataset. \n",
    "\n",
    "The Test dataset provides the gold standard used to evaluate the model. It is only used once a model is completely trained(using the train and validation sets). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b7ead4-c697-4e40-a18c-5b33118a2423",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "![train val test](https://miro.medium.com/max/1400/1*Nv2NNALuokZEcV6hYEHdGA.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d83b254-585e-4ac3-b11f-0f3735485864",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ebebbca-b3bf-49d9-b940-036d1e294492",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0, 1],\n",
       "        [2, 3],\n",
       "        [4, 5],\n",
       "        [6, 7],\n",
       "        [8, 9]]),\n",
       " range(0, 5))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = np.arange(10).reshape((5, 2)), range(5)\n",
    "X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a9051b1-f1ce-449a-8fb1-a3715cd87acd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[4, 5],\n",
       "        [0, 1],\n",
       "        [6, 7]]),\n",
       " array([[2, 3],\n",
       "        [8, 9]]),\n",
       " [2, 0, 3],\n",
       " [1, 4])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a66334-6797-499d-a1ea-dbb1c9523f8b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## References \n",
    "\n",
    "* [Train, validation and test sets](https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7)\n",
    "* [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21edefcd-ed26-4756-b88f-3b4bf2697823",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d2a956-92dc-4b8a-8912-9eb778a42292",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "## MNIST\n",
    "\n",
    "The MNIST database of handwritten digits, available from this page, has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST.\n",
    "\n",
    "![mnist](https://upload.wikimedia.org/wikipedia/commons/thumb/2/27/MnistExamples.png/320px-MnistExamples.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655e7e11-2bef-42c3-8602-4feb17db640d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### References\n",
    "\n",
    "* Kussul, Ernst; Baidyk, Tatiana (2004). \"Improved method of handwritten digit recognition tested on MNIST database\". Image and Vision Computing. 22 (12): 971–981. [doi:10.1016/j.imavis.2004.03.008](doi:10.1016/j.imavis.2004.03.008) .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b06af7-a5f2-4bf8-930d-05599b29f5b2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# ImageNet\n",
    "\n",
    "The most highly-used subset of ImageNet is the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2012-2017 image classification and localization dataset. This dataset spans 1000 object classes and contains 1,281,167 training images, 50,000 validation images and 100,000 test images. This subset is available on Kaggle.\n",
    "\n",
    "![tengi](https://www.researchgate.net/profile/Tengqi-Ye/publication/324797660/figure/fig17/AS:619962838315022@1524822067263/Figure-A1-A-snapshot-of-two-root-to-leaf-branches-of-ImageNet-the-top-row-is-from-the.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74aacc71-3c7f-4a54-9a61-18f255646c55",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "On 30 September 2012, a convolutional neural network (CNN) called AlexNet achieved a top-5 error of 15.3% in the ImageNet 2012 Challenge, more than 10.8 percentage points lower than that of the runner up. This was made feasible due to the use of graphics processing units (GPUs) during training, an essential ingredient of the deep learning revolution. According to The Economist, \"Suddenly people started to pay attention, not just within the AI community but across the technology industry as a whole.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04eac1d-3302-471f-9c87-1eb1ec02e700",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## References\n",
    "* J. Deng, W. Dong, R. Socher, L. -J. Li, Kai Li and Li Fei-Fei, \"ImageNet: A large-scale hierarchical image database,\" 2009 IEEE Conference on Computer Vision and Pattern Recognition, 2009, pp. 248-255, doi: [10.1109/CVPR.2009.5206848](https://doi.org/10.1109/CVPR.2009.5206848).\n",
    "* [Wikipedia: ImageNet](https://en.wikipedia.org/wiki/ImageNet)\n",
    "* Krizhevsky, Alex; Sutskever, Ilya; Hinton, Geoffrey E. (June 2017). \"ImageNet classification with deep convolutional neural networks\" (PDF). Communications of the ACM. 60 (6): 84–90. [doi:10.1145/3065386](https://doi.org/10.1145%2F3065386). ISSN 0001-0782. S2CID 195908774. Retrieved 24 May 2017.\n",
    "* [Ye Tengqi, \"Visual Object Detection from Lifelogs using Visual Non-lifelog Data\"](http://dx.doi.org/10.13140/RG.2.2.18463.46248)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8248ad-2440-464b-9c0d-fde9b9d771d6",
   "metadata": {},
   "source": [
    "## COCO\n",
    "\n",
    "* Object segmentation\n",
    "* Recognition in context\n",
    "* Superpixel stuff segmentation\n",
    "* 330K images (>200K labeled)\n",
    "* 1.5 million object instances\n",
    "* 80 object categories\n",
    "* 91 stuff categories\n",
    "* 5 captions per image\n",
    "* 250,000 people with keypoints\n",
    "\n",
    "![coco](https://cocodataset.org/images/coco-examples.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9863cdf4-6461-469a-95c0-b317bc4122fe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<img src=\"non-iconic_coco.png\" width=800/>\n",
    "\n",
    "Source: Microsoft COCO: Common Objects in Context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43b8b23-827b-4671-b207-583473f3a13f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## COCO json\n",
    "\n",
    "\n",
    "COCO format JSON file consists of five sections providing information for an entire dataset. For more information, see COCO format.\n",
    "\n",
    "* info – general information about the dataset.\n",
    "* licenses – license information for the images in the dataset.\n",
    "* images – a list of images in the dataset.\n",
    "* annotations – a list of annotations (including bounding boxes) that are present in all images in the dataset.\n",
    "* categories – a list of label categories.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162be86f-d0be-4a3b-8d02-039b4240563b",
   "metadata": {},
   "source": [
    "```json\n",
    "{\n",
    "    \"info\": info, \n",
    "    \"images\": [image], \n",
    "    \"annotations\": [annotation], \n",
    "    \"licenses\": [license],\n",
    "}\n",
    "info{\n",
    "    \"year\": int, \n",
    "    \"version\": str, \n",
    "    \"description\": str, \n",
    "    \"contributor\": str, \n",
    "    \"url\": str, \n",
    "    \"date_created\": datetime,\n",
    "}\n",
    "image{\n",
    "    \"id\": int, \n",
    "    \"width\": int,\n",
    "    \"height\": int,\n",
    "    \"file_name\": str,\n",
    "    \"license\": int,\n",
    "    \"flickr_url\": str,\n",
    "    \"coco_url\": str,\n",
    "    \"date_captured\": datetime,\n",
    "}\n",
    "license{\n",
    "    \"id\": int, \n",
    "    \"name\": str, \n",
    "    \"url\": str,\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37713166-2d16-4f3a-b58d-12dd7ad75f2b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "```json\n",
    "{\n",
    "    \"info\": {\n",
    "        \"description\": \"COCO 2017 Dataset\",\n",
    "        \"url\": \"http://cocodataset.org\",\n",
    "        \"version\": \"1.0\",\"year\": 2017,\n",
    "        \"contributor\": \"COCO Consortium\",\n",
    "        \"date_created\": \"2017/09/01\"\n",
    "    },\n",
    "    \"licenses\": [\n",
    "        {\"url\": \"http://creativecommons.org/licenses/by/2.0/\",\"id\": 4,\"name\": \"Attribution License\"}\n",
    "    ],\n",
    "    \"images\": [\n",
    "        {\"id\": 242287, \"license\": 4, \n",
    "         \"coco_url\": \"http://images.cocodataset.org/val2017/xxxxxxxxxxxx.jpg\",\n",
    "         \"flickr_url\": \"http://farm3.staticflickr.com/2626/xxxxxxxxxxxx.jpg\",\n",
    "         \"width\": 426, \"height\": 640, \"file_name\": \"xxxxxxxxx.jpg\", \n",
    "         \"date_captured\": \"2013-11-15 02:41:42\"},\n",
    "        {\"id\": 245915, \"license\": 4, \n",
    "         \"coco_url\": \"http://images.cocodataset.org/val2017/nnnnnnnnnnnn.jpg\",\n",
    "         \"flickr_url\": \"http://farm1.staticflickr.com/88/xxxxxxxxxxxx.jpg\", \n",
    "         \"width\": 640, \"height\": 480, \"file_name\": \"nnnnnnnnnn.jpg\",\n",
    "         \"date_captured\": \"2013-11-18 02:53:27\"}\n",
    "    ],\n",
    "    \"annotations\": [\n",
    "        {\"id\": 125686, \"category_id\": 0, \"iscrowd\": 0, \n",
    "         \"segmentation\": [[164.81, 417.51,......167.55, 410.64]], \n",
    "         \"image_id\": 242287, \"area\": 42061.80340000001, \n",
    "         \"bbox\": [19.23, 383.18, 314.5, 244.46]},\n",
    "        {\"id\": 1409619, \"category_id\": 0, \"iscrowd\": 0, \n",
    "         \"segmentation\": [[376.81, 238.8,........382.74, 241.17]], \n",
    "         \"image_id\": 245915, \"area\": 3556.2197000000015, \n",
    "         \"bbox\": [399, 251, 155, 101]},\n",
    "        {\"id\": 1410165, \"category_id\": 1, \"iscrowd\": 0, \n",
    "         \"segmentation\": [[486.34, 239.01,..........495.95, 244.39]], \n",
    "         \"image_id\": 245915, \"area\": 1775.8932499999994, \n",
    "         \"bbox\": [86, 65, 220, 334]}\n",
    "    ],\n",
    "    \"categories\": [\n",
    "        {\"supercategory\": \"speaker\",\"id\": 0,\"name\": \"echo\"},\n",
    "        {\"supercategory\": \"speaker\",\"id\": 1,\"name\": \"echo dot\"}\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0506f0f2-83ce-4a70-bd15-0812e8bee4b2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Resources\n",
    "\n",
    "\n",
    "* [Paper: \"Microsoft COCO: Common Objects in Context\"](https://link.springer.com/content/pdf/10.1007%2F978-3-319-10602-1_48.pdf)\n",
    "* [COCO json format](https://cocodataset.org/#format-data)\n",
    "* [Transforming COCO datasets](https://docs.aws.amazon.com/rekognition/latest/customlabels-dg/md-transform-coco.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6162515d-ff8e-46d6-a346-4dbf1b2b37ad",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Few other datasets..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82076f25-19c2-4ae8-b47d-2193a2a83c31",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Skin cancer dataset HAM10000\n",
    "\n",
    "<img src=\"https://www.researchgate.net/publication/341264074/figure/fig1/AS:889106708058114@1588990966693/Examples-of-images-downloaded-from-the-HAM10000-dataset-These-images-are-publicly.png\" width=600>\n",
    "\n",
    "Examples of images downloaded from the HAM10000 dataset. These images are publicly available through the International Skin Imaging Collaboration (ISIC) archive and represent more than 95% of all pigmented lesions encountered during clinical practice (Tschandl P 2018). (A) Melanocytic nevus; (B) Bening keratosis; (C) Vascular lesion; (D) Dermatofibroma; (E) Intraepithelial carcinoma; (F) Basal cell carcinoma; (G) Melanoma. Legends inside each image represents clinical data such as age, sex and localization associated to the image. F: female; M: male; LE: lower extremity; B: back; H: Hand; T: trunk.\n",
    "\n",
    "Source: [Deep Neural Frameworks Improve the Accuracy of General Practitioners in the Classification of Pigmented Skin Lesions](https://doi.org/10.3390/diagnostics10110969)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e124757-a638-48d5-8246-9782395846e4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Liver Segmentation 3D-IRCADb-01\n",
    "\n",
    "![ircad](https://www.ircad.fr/wp-content/uploads/3Dircadb1_package.jpeg)\n",
    "\n",
    "[IRCADb](https://www.ircad.fr/research/data-sets/liver-segmentation-3d-ircadb-01/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3187c4-96ec-4a6d-adf1-0beb6c2e5a6e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Annotation tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019ee6fb-de64-46eb-a953-051f10b49731",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## CVAT \n",
    "\n",
    "![cvat](https://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Cvat_%282%29.jpg/450px-Cvat_%282%29.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4500db19-7577-4e80-9ea0-6ec8474c7064",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Via annotation tool\n",
    "![via](https://www.robots.ox.ac.uk/~vgg/software/via/images/via_video_annotator.png)\n",
    "\n",
    "[via annotation tool](https://www.robots.ox.ac.uk/~vgg/software/via/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9138553-f68f-4c1e-b5a1-a7ed36adf6a4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# End to end training\n",
    "\n",
    "![traditional cv vs deep learning](https://www.researchgate.net/profile/Niall-O-Mahony/publication/331586553/figure/fig1/AS:753501407809536@1556660143025/a-Traditional-Computer-Vision-workflow-vs-b-Deep-Learning-workflow-Figure-from-8.jpg)\n",
    "\n",
    "Taken from \"Deep Learning vs. Traditional Computer Vision\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be726d89-f693-40b8-8b07-5563393ff276",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "![Building block of a CNN](https://www.researchgate.net/profile/Niall-O-Mahony/publication/331586553/figure/fig2/AS:753501407817728@1556660143057/Building-blocks-of-a-CNN-Figure-from-13.ppm)\n",
    "\n",
    "\n",
    "Taken from \"Deep Learning vs. Traditional Computer Vision\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a060aa-df0e-47c3-b210-16f65507a9f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81822ae4-0602-4c53-8fe0-8d67ceeaa668",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Deep learning\n",
    "\n",
    "[Teachable Machine](https://teachablemachine.withgoogle.com/)\n",
    "\n",
    "[Detectron2 in Google Colab](https://colab.research.google.com/github/Tony607/detectron2_instance_segmentation_demo/blob/master/Detectron2_custom_coco_data_segmentation.ipynb#scrollTo=MWknKqWTWIw9)\n",
    "\n",
    "[Detetron2 on Windows](https://github.com/mjirik/tutorials/tree/main/detectron_windows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96002a3-7755-45b6-a574-1ee490ca2ad0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## References\n",
    "* [Deep Learning vs. Traditional Computer Vision](http://dx.doi.org/10.1007/978-3-030-17795-9_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c0335c-fc91-41f4-a0c1-c76a8e6e4e1b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Metacentrum\n",
    "\n",
    "[Beginners Guide](https://wiki.metacentrum.cz/wiki/Beginners_guide)\n",
    "\n",
    "[Jupyter for MetaCentrum Users](https://wiki.metacentrum.cz/wiki/Jupyter_for_MetaCentrum_users)\n",
    "\n",
    "[Detectron2 on MetaCentrum](https://github.com/mjirik/tutorials/tree/main/metacentrum/detectron2_quickstart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a86b7e-5cd7-4a22-80fe-1f12ee8c4e24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
